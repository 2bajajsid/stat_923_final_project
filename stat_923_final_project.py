# -*- coding: utf-8 -*-
"""stat_923_final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103D6JPi9Lym3Ez57T05tOkQRSenMXLnQ
"""

import pandas as pd
import numpy as np
amzn_commerce_reviews_dataset = pd.read_csv('amazon-commerce-reviews.csv')
np.random.seed(923)

"""Sparsity of our Dataset"""

sparsity = 1 - (np.count_nonzero(amzn_commerce_reviews_dataset.iloc[:, 0:10000]) / amzn_commerce_reviews_dataset.iloc[:, 0:10000].size)
print(sparsity)

"""Our data matrix is highly-sparse!!

20% of the Dataset is reserved as a validation data-set and the rest is the training dataset.
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(amzn_commerce_reviews_dataset.iloc[:, 0:10000],
                                                    amzn_commerce_reviews_dataset.iloc[:, 10000],
                                                    stratify=amzn_commerce_reviews_dataset.iloc[:, 10000],
                                                    test_size=0.2)

# Let's make sure we have a balanced training set.
print(y_train.value_counts())

"""We encode the labels of our classes so that they are mapped to {0, 1, ..., 49}"""

import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y_train = le.fit_transform(y_train)
y_test = le.fit_transform(y_test)
print(np.unique(y_train))

"""Let's do some PCA!"""

# We must scale our datasets first to avoid having features of different magnitudes interfere
# with any dimensionality reduction methods or even the downstream classification models' performance
from sklearn.preprocessing import MaxAbsScaler
scaler = MaxAbsScaler().fit(X_train)
X_train, X_test = scaler.transform(X_train), scaler.transform(X_test)

from sklearn.decomposition import PCA
pca = PCA(n_components=1200)
X_train_scores_pca = pca.fit_transform(X_train)
X_test_scores_pca = pca.transform(X_test)

import matplotlib.pyplot as plt
plt.scatter(X_train_scores_pca[:, 0], X_train_scores_pca[:, 1],
            c=y_train, edgecolor='none', alpha=0.5,
            cmap=plt.cm.get_cmap('Accent', 30))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.title("Fig. 1 PC Scores along the first and second principal components")
plt.colorbar();

plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')
plt.ylabel('cumulative explained variance');
plt.title('Fig.2 Number of components vs Cumulative Explained Variance')

X_train_scores_pca = X_train_scores_pca[:, :600]
X_test_scores_pca = X_test_scores_pca[:, :600]

"""Let's do some Neighborhood Component Analysis (NCA)"""

from sklearn.neighbors import NeighborhoodComponentsAnalysis
nca = NeighborhoodComponentsAnalysis(random_state=923, n_components=10)
nca.fit(X_train, y_train)
X_train_scores_nca = nca.transform(X_train)
X_test_scores_nca = nca.transform(X_test)

plt.scatter(X_train_scores_nca[:, 0], X_train_scores_nca[:, 1],
            c=y_train, edgecolor='none', alpha=0.5,
            cmap=plt.cm.get_cmap('Accent', 30))
plt.xlabel('component 1')
plt.ylabel('component 2')
plt.title("Fig. 3 NCA projections along the first two components")
plt.colorbar();

# However, we will fit NCA with 600 components for parity purposes with PCA
nca = NeighborhoodComponentsAnalysis(random_state=923, n_components=600)
nca.fit(X_train, y_train)
X_train_scores_nca = nca.transform(X_train)
X_test_scores_nca = nca.transform(X_test)

"""Chi-square test for independence"""

from sklearn.feature_selection import chi2
chi2_stats, p_values = chi2(X_train, y_train)

# calculate the number of statistically significant variables after applying the Bonferroni's correction
num_signficant_variables = sum(i <= (0.05 / 10000) for i in p_values)
print(num_signficant_variables)

statistically_significant_indices = np.argpartition(p_values, 303)[:303]
X_train_chi_square=X_train[:, statistically_significant_indices]
X_test_chi_square=X_test[:, statistically_significant_indices]

"""Method #1 Naive Bayes"""

from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.model_selection import cross_val_score

# Let's first try fitting Multinomial_NBA on the unreduced dataset
# where we transform the training data set back into the integer counts
# since the multinomial NB works best with discrete features such as counts
multinomial_nb = MultinomialNB()
multinomial_nb_pca_scores = cross_val_score(multinomial_nb, scaler.inverse_transform(X_train), y_train, cv=5)
print(f'The average cross validation accuracy score is: {np.mean(multinomial_nb_pca_scores)}.')
print(f'The validation data score is: {multinomial_nb.fit(scaler.inverse_transform(X_train), y_train).score(scaler.inverse_transform(X_test), y_test)}.')

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

predictions = multinomial_nb.predict(X_test)
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = multinomial_nb.classes_)

# Only 6 out of 50 classes have an accuracy percentage of under 50% on the test set
print(sum(np.diagonal(cm) < 3))

fig, ax = plt.subplots(figsize=(11, 11))
disp.plot(ax=ax)

# Now, let's try fitting GaussianNB on the PCA scores
gaussian_nb = GaussianNB()
gaussian_nb_pca_scores = cross_val_score(gaussian_nb, X_train_scores_pca, y_train, cv=5)
print(f'The average cross validation accuracy score is: {np.mean(gaussian_nb_pca_scores)}.')
print(f'The validation data score is: {gaussian_nb.fit(X_train_scores_pca, y_train).score(X_test_scores_pca, y_test)}.')

# Now, let's try fitting GaussianNB on the NCA projections
gaussian_nb = GaussianNB()
gaussian_nb_nca_scores = cross_val_score(gaussian_nb, X_train_scores_nca, y_train, cv=5)
print(f'The average cross validation accuracy score is: {np.mean(gaussian_nb_nca_scores)}.')
print(f'The validation data score is: {gaussian_nb.fit(X_train_scores_nca, y_train).score(X_test_scores_nca, y_test)}.')

# Now, let's try fitting GaussianNB on the chi-square selected features
multinomial_nb = MultinomialNB()
multinomial_nb_chi_square_scores = cross_val_score(multinomial_nb, scaler.inverse_transform(X_train)[:, statistically_significant_indices], y_train, cv=5)
print(f'The average cross validation accuracy score is: {np.mean(multinomial_nb_chi_square_scores)}.')
print(f'The validation data score is: {multinomial_nb.fit(scaler.inverse_transform(X_train)[:,  statistically_significant_indices], y_train).score(scaler.inverse_transform(X_test)[:,  statistically_significant_indices], y_test)}.')

"""Method #2 K-Nearest Neighbors"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
params = {'n_neighbors': [1, 2, 3, 5], 'weights': ['uniform', 'distance']}

# Let's try fitting k-nearest-neighbors using grid-search CV on PCA scores first
knn = KNeighborsClassifier()
grid_search_knn_pca = GridSearchCV(knn, params, cv=5)
grid_search_knn_pca.fit(X_train_scores_pca, y_train)

print(grid_search_knn_pca.best_score_)
print(grid_search_knn_pca.best_params_)
grid_search_knn_pca.best_estimator_.score(X_test_scores_pca, y_test)

# Let's try fitting k-nearest-neighbors using grid-search CV on NCA projections
knn_nca = KNeighborsClassifier()
grid_search_knn_nca = GridSearchCV(knn_nca, params, cv=5)
grid_search_knn_nca.fit(X_train_scores_nca, y_train)

print(grid_search_knn_nca.best_score_)
print(grid_search_knn_nca.best_params_)
grid_search_knn_nca.best_estimator_.score(X_test_scores_nca, y_test)

# Let's try fitting k-nearest-neighbors using grid-search CV on the chi-square selected variables
knn_chi_square = KNeighborsClassifier()
grid_search_chi_square = GridSearchCV(knn_chi_square, params, cv=5)
grid_search_chi_square.fit(X_train[:, statistically_significant_indices], y_train)

print(grid_search_chi_square.best_score_)
print(grid_search_chi_square.best_params_)
grid_search_chi_square.best_estimator_.score(X_test[:, statistically_significant_indices], y_test)

# Let's try fitting k-nearest-neighbors using grid-search CV on the original, scaled dataset
knn_original_scaled = KNeighborsClassifier()
grid_search_original_scaled = GridSearchCV(knn_original_scaled, params, cv=5)
grid_search_original_scaled.fit(X_train, y_train)

print(grid_search_original_scaled.best_score_)
print(grid_search_original_scaled.best_params_)
grid_search_original_scaled.best_estimator_.score(X_test, y_test)

predictions = grid_search_knn_nca.best_estimator_.predict(X_test_scores_nca)
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = grid_search_knn_nca.best_estimator_.classes_)
fig, ax = plt.subplots(figsize=(11, 11))
disp.plot(ax=ax)

# Approximately 18 out of 50 classes have an accuracy percentage of under 50% on the test set
print(sum(np.diagonal(cm) < 3))

"""Method #3 SVC"""

from sklearn.svm import SVC
params = {'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
          'gamma': ['scale', 'auto'] }

# Let's try fitting SVMS using grid-search CV on PCA scores first
svm = SVC()
grid_search_svm_pca = GridSearchCV(svm, params, cv=5)
grid_search_svm_pca.fit(X_train_scores_pca, y_train)

print(grid_search_svm_pca.best_score_)
print(grid_search_svm_pca.best_params_)
grid_search_svm_pca.best_estimator_.score(X_test_scores_pca, y_test)

# Let's try fitting SVMS using grid-search CV on NCA projections
svm = SVC()
grid_search_svm_nca = GridSearchCV(svm, params, cv=5)
grid_search_svm_nca.fit(X_train_scores_nca, y_train)

print(grid_search_svm_nca.best_score_)
print(grid_search_svm_nca.best_params_)
grid_search_svm_nca.best_estimator_.score(X_test_scores_nca, y_test)

# Let's try fitting SVMS using grid-search CV on the chi-square selected variables
svm = SVC()
grid_search_svm_chi_square = GridSearchCV(svm, params, cv=5)
grid_search_svm_chi_square.fit(X_train[:, statistically_significant_indices], y_train)

print(grid_search_svm_chi_square.best_score_)
print(grid_search_svm_chi_square.best_params_)
grid_search_svm_chi_square.best_estimator_.score(X_test[:, statistically_significant_indices], y_test)

# Let's try fitting SVMS using grid-search CV on all the features
svm = SVC()
grid_search_svm_original = GridSearchCV(svm, params, cv=5)
grid_search_svm_original.fit(X_train, y_train)

print(grid_search_svm_original.best_score_)
print(grid_search_svm_original.best_params_)
grid_search_svm_original.best_estimator_.score(X_test, y_test)

predictions = grid_search_svm_original.best_estimator_.predict(X_test)
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = grid_search_svm_original.best_estimator_.classes_)
fig, ax = plt.subplots(figsize=(11, 11))
disp.plot(ax=ax)

# Approximately 3 out of 50 classes have an accuracy percentage of under 50% on the test set
print(sum(np.diagonal(cm) < 3))

"""Method #4 Logistic Regression"""

from sklearn.linear_model import LogisticRegression
 params = {'penalty': ['l2', 'l1', 'elasticnet', 'none']}

logitReg = LogisticRegression(solver='saga')
grid_search_logit_reg_pca = GridSearchCV(logitReg, params, cv=5)
grid_search_logit_reg_pca.fit(X_train_scores_pca, y_train)

print(grid_search_logit_reg_pca.best_score_)
print(grid_search_logit_reg_pca.best_params_)
grid_search_logit_reg_pca.best_estimator_.score(X_test_scores_pca, y_test)

#Fitting Logistic Regression on nca
logitReg = LogisticRegression(solver='saga')
grid_search_logit_reg_nca = GridSearchCV(logitReg, params, cv=5)
grid_search_logit_reg_nca.fit(X_train_scores_nca, y_train)

print(grid_search_logit_reg_nca.best_score_)
print(grid_search_logit_reg_nca.best_params_)
grid_search_logit_reg_nca.best_estimator_.score(X_test_scores_nca, y_test)

#Fitting Logistic Regression on chi-square
logitReg = LogisticRegression(solver='saga')
grid_search_logit_reg_chi = GridSearchCV(logitReg, params, cv=5)
grid_search_logit_reg_chi.fit(X_train[:, statistically_significant_indices], y_train)

print(grid_search_logit_reg_chi.best_score_)
print(grid_search_logit_reg_chi.best_params_)
grid_search_logit_reg_chi.best_estimator_.score(X_test[:, statistically_significant_indices], y_test)

#Fitting Logistic Regression on the original, scaled dataset
logitReg = LogisticRegression(solver='saga')
grid_search_logit_reg_original = GridSearchCV(logitReg, params, cv=5)
grid_search_logit_reg_original.fit(X_train, y_train)

print(grid_search_logit_reg_original.best_score_)
print(grid_search_logit_reg_original.best_params_)
grid_search_logit_reg_original.best_estimator_.score(X_test, y_test)

predictions = grid_search_logit_reg_original.best_estimator_.predict(X_test)
cm = confusion_matrix(y_test, predictions)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels = grid_search_logit_reg_original.best_estimator_.classes_)
fig, ax = plt.subplots(figsize=(11, 11))
disp.plot(ax=ax)

print(sum(np.diagonal(cm) < 3))

"""Method #5 XG-Boost"""

import xgboost as xgb
params = {
        'subsample': [0.6, 0.8, 1.0],
        'colsample_bytree': [0.6, 0.8, 1.0],
        'learning_rate': [0.02, 0.1],
        }

# First, we fit the xgboost model on the 303 statistically significant variables
clf_chi = xgb.XGBClassifier(n_estimators=50, gamma=0.5, max_depth=3)
grid_search_xg_chi = GridSearchCV(clf_chi, params, refit=True, cv = 5, verbose=2)
grid_search_xg_chi.fit(X_train[:, statistically_significant_indices], y_train)

print(grid_search_xg_chi.best_score_)
print(grid_search_xg_chi.best_params_)
grid_search_xg_chi.best_estimator_.score(X_test[:, statistically_significant_indices], y_test)

# Now, we fit the xgboost model on the PCA scores
clf_pca = xgb.XGBClassifier(n_estimators=50, gamma=0.5, max_depth=3, n_jobs=-1)
grid_search_xg_pca = GridSearchCV(clf_pca, params, refit=True, cv = 2, verbose=2)
grid_search_xg_pca.fit(X_train_scores_pca, y_train)

print(grid_search_xg_pca.best_score_)
print(grid_search_xg_pca.best_params_)
grid_search_xg_pca.best_estimator_.score(X_test_scores_pca, y_test)

# Now, we fit the xgboost model on the original data set
clf_original = xgb.XGBClassifier(n_estimators=50, gamma=0.5, max_depth=4, n_jobs=-1)
grid_search_xg_original = GridSearchCV(clf_original, params, refit=True, cv = 3, verbose=2)
grid_search_xg_original.fit(X_train, y_train)

print(grid_search_xg_original.best_score_)
print(grid_search_xg_original.best_params_)
grid_search_xg_original.best_estimator_.score(X_test, y_test)

"""Method #6 Neural Networks"""

import os
import torch
from torch import nn

def encode_one_hot(category):
    listofzeros = [0] * 50
    listofzeros[category] = 1
    return listofzeros

def decode_one_hot(listofzeros):
    category = np.argmax(listofzeros)
    return category

class NeuralNetwork(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(10000, 1000),
            nn.ReLU(),
            nn.Dropout(0.25),
            nn.Linear(1000, 256),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(256, 50),
            nn.ReLU(),
        )

    def forward(self, x):
        logits = self.linear_relu_stack(x)
        return logits

# define loss, optimizier
model = NeuralNetwork().double()
optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)
criterion = torch.nn.CrossEntropyLoss()

# Train the model
Loss = []
TestSetPerformance = []
epochs = 150
for epoch in range(epochs):
    Loss.append(0)
    TestSetPerformance.append(0)
    print(epoch)
    random_indices = np.random.randint(len(X_train), size=250)
    for i in random_indices:
        x = torch.tensor(X_train[i]).double()
        y = torch.tensor(encode_one_hot(y_train[i]))
        optimizer.zero_grad()
        y_pred = model(x)
        loss = criterion(y_pred.float(), y.float())
        Loss[epoch] += loss
        loss.backward()
        optimizer.step()

    test_set_misclassifications = 0
    for i in range(len(X_test)):
      y_pred = model(torch.tensor(X_test[i]).double())
      y_pred = decode_one_hot(y_pred.detach().numpy())
      if y_pred != y_test[i]:
        test_set_misclassifications += 1
    TestSetPerformance[epoch] = 1 - (test_set_misclassifications / len(X_test))

    print(Loss[epoch])
    print(TestSetPerformance[epoch])
    print("**")

def getItemFromTensorObject(x):
  return x.item()
plt.plot(list(map(getItemFromTensorObject, Loss)))
plt.xlabel('Epoch Number')
plt.ylabel('Training Loss [Categorical Cross Entropy]')
plt.show()

plt.plot(TestSetPerformance)
plt.xlabel('Epoch Number')
plt.ylabel('Test Set Accuracy')
plt.show()